{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Word Prediction\n",
    "\n",
    "About:\n",
    "\n",
    "The project aims to develop a deep learning model for predicting the next word in a given sequence of words. The model is built using LSTM [Long Short Term Memory] networks, which are suited for sequence prediction tasks.\n",
    "\n",
    "1) Data Collection: we will use Shakespeare's Hamlet as our dataset.\n",
    "\n",
    "2) Data Preprocessing: The text data is tokenized, converted to sequences, and are padded to ensure uniform input lengths. Then we split into train and test data.\n",
    "\n",
    "3) Model Building: an LSTM is constructed with an embedding layer, two LSTM layers, and a dense output layer with a softmax function to predict the probablity of the next word.\n",
    "\n",
    "4) Model Training: the model is trained using the prepared sequences, with early stopping implemented to prevent overfitting. Early stopping monitors the validation loss and stops trainings when the loss stops improving.\n",
    "\n",
    "5) Model Evaluation: a set of examples are prepared to test model ability to predict the next sequence word.\n",
    "\n",
    "6) Deployment: A streamlit web application is developed to allow users to input sequence of words and predict the next word in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to C:\\Users\\Nitin\n",
      "[nltk_data]     Flavier\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "data = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "\n",
    "with open('../data/hamlet.txt','w') as file_obj:\n",
    "    file_obj.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Nitin Flavier\\Desktop\\Data Nexus\\Data Science\\ML_BootCamp\\NLP_Deep_Learning\\Next_Word_Prediction_LSTM\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'to': 3,\n",
       " 'of': 4,\n",
       " 'i': 5,\n",
       " 'you': 6,\n",
       " 'a': 7,\n",
       " 'my': 8,\n",
       " 'it': 9,\n",
       " 'in': 10,\n",
       " 'that': 11,\n",
       " 'ham': 12,\n",
       " 'is': 13,\n",
       " 'not': 14,\n",
       " 'his': 15,\n",
       " 'this': 16,\n",
       " 'with': 17,\n",
       " 'your': 18,\n",
       " 'but': 19,\n",
       " 'for': 20,\n",
       " 'me': 21,\n",
       " 'lord': 22,\n",
       " 'as': 23,\n",
       " 'what': 24,\n",
       " 'he': 25,\n",
       " 'be': 26,\n",
       " 'so': 27,\n",
       " 'him': 28,\n",
       " 'haue': 29,\n",
       " 'king': 30,\n",
       " 'will': 31,\n",
       " 'no': 32,\n",
       " 'our': 33,\n",
       " 'we': 34,\n",
       " 'on': 35,\n",
       " 'are': 36,\n",
       " 'if': 37,\n",
       " 'all': 38,\n",
       " 'then': 39,\n",
       " 'shall': 40,\n",
       " 'by': 41,\n",
       " 'thou': 42,\n",
       " 'come': 43,\n",
       " 'or': 44,\n",
       " 'hamlet': 45,\n",
       " 'good': 46,\n",
       " 'do': 47,\n",
       " 'hor': 48,\n",
       " 'her': 49,\n",
       " 'let': 50,\n",
       " 'now': 51,\n",
       " 'thy': 52,\n",
       " 'how': 53,\n",
       " 'more': 54,\n",
       " 'they': 55,\n",
       " 'from': 56,\n",
       " 'enter': 57,\n",
       " 'at': 58,\n",
       " 'was': 59,\n",
       " 'oh': 60,\n",
       " 'like': 61,\n",
       " 'most': 62,\n",
       " 'there': 63,\n",
       " 'well': 64,\n",
       " 'know': 65,\n",
       " 'selfe': 66,\n",
       " 'would': 67,\n",
       " 'them': 68,\n",
       " 'loue': 69,\n",
       " 'may': 70,\n",
       " \"'tis\": 71,\n",
       " 'vs': 72,\n",
       " 'sir': 73,\n",
       " 'qu': 74,\n",
       " 'which': 75,\n",
       " 'did': 76,\n",
       " 'why': 77,\n",
       " 'laer': 78,\n",
       " 'giue': 79,\n",
       " 'thee': 80,\n",
       " 'ile': 81,\n",
       " 'must': 82,\n",
       " 'hath': 83,\n",
       " 'ophe': 84,\n",
       " 'speake': 85,\n",
       " 'out': 86,\n",
       " 'make': 87,\n",
       " 'should': 88,\n",
       " 'where': 89,\n",
       " 'too': 90,\n",
       " 'an': 91,\n",
       " 'am': 92,\n",
       " 'such': 93,\n",
       " 'say': 94,\n",
       " 'when': 95,\n",
       " 'vpon': 96,\n",
       " 'father': 97,\n",
       " 'doe': 98,\n",
       " 'very': 99,\n",
       " 'pol': 100,\n",
       " 'go': 101,\n",
       " 'their': 102,\n",
       " 'one': 103,\n",
       " 'man': 104,\n",
       " 'see': 105,\n",
       " 'some': 106,\n",
       " 'heere': 107,\n",
       " 'had': 108,\n",
       " 'heauen': 109,\n",
       " 'time': 110,\n",
       " 'mine': 111,\n",
       " 'these': 112,\n",
       " 'she': 113,\n",
       " 'much': 114,\n",
       " 'tell': 115,\n",
       " 'rosin': 116,\n",
       " 'thinke': 117,\n",
       " 'play': 118,\n",
       " 'thus': 119,\n",
       " 'horatio': 120,\n",
       " 'who': 121,\n",
       " 'mother': 122,\n",
       " 'queene': 123,\n",
       " 'night': 124,\n",
       " 'o': 125,\n",
       " 'polon': 126,\n",
       " 'yet': 127,\n",
       " 'vp': 128,\n",
       " 'death': 129,\n",
       " 'laertes': 130,\n",
       " 'againe': 131,\n",
       " 'can': 132,\n",
       " 'both': 133,\n",
       " \"th'\": 134,\n",
       " 'soule': 135,\n",
       " 'take': 136,\n",
       " 'life': 137,\n",
       " 'nor': 138,\n",
       " 'heare': 139,\n",
       " 'mar': 140,\n",
       " 'looke': 141,\n",
       " 'owne': 142,\n",
       " 'could': 143,\n",
       " 'heart': 144,\n",
       " 'dead': 145,\n",
       " 'might': 146,\n",
       " 'made': 147,\n",
       " 'clo': 148,\n",
       " 'hast': 149,\n",
       " 'downe': 150,\n",
       " 'pray': 151,\n",
       " 'ophelia': 152,\n",
       " 'nothing': 153,\n",
       " 'away': 154,\n",
       " 'whose': 155,\n",
       " 'doth': 156,\n",
       " 'other': 157,\n",
       " 'cannot': 158,\n",
       " 'leaue': 159,\n",
       " 'indeed': 160,\n",
       " 'into': 161,\n",
       " 'nay': 162,\n",
       " 'god': 163,\n",
       " 'head': 164,\n",
       " 'were': 165,\n",
       " 'matter': 166,\n",
       " 'thing': 167,\n",
       " 'hold': 168,\n",
       " 'day': 169,\n",
       " 'world': 170,\n",
       " 'nature': 171,\n",
       " 'neuer': 172,\n",
       " 'comes': 173,\n",
       " 'done': 174,\n",
       " 'exeunt': 175,\n",
       " 'call': 176,\n",
       " 'two': 177,\n",
       " 'true': 178,\n",
       " 'though': 179,\n",
       " 'sweet': 180,\n",
       " 'put': 181,\n",
       " 'set': 182,\n",
       " 'ghost': 183,\n",
       " 'euen': 184,\n",
       " 'earth': 185,\n",
       " 'feare': 186,\n",
       " 'madnesse': 187,\n",
       " 'mad': 188,\n",
       " 'seene': 189,\n",
       " 'eyes': 190,\n",
       " 'against': 191,\n",
       " 'faire': 192,\n",
       " 'denmarke': 193,\n",
       " 'those': 194,\n",
       " \"o're\": 195,\n",
       " 'polonius': 196,\n",
       " 'deere': 197,\n",
       " 'fathers': 198,\n",
       " 'sonne': 199,\n",
       " 'poore': 200,\n",
       " 'himselfe': 201,\n",
       " 'follow': 202,\n",
       " 'guild': 203,\n",
       " 'england': 204,\n",
       " 'friends': 205,\n",
       " 'once': 206,\n",
       " 'hand': 207,\n",
       " 'shew': 208,\n",
       " 'about': 209,\n",
       " \"i'th'\": 210,\n",
       " 'off': 211,\n",
       " 'within': 212,\n",
       " 'till': 213,\n",
       " 'great': 214,\n",
       " 'meanes': 215,\n",
       " 'words': 216,\n",
       " 'players': 217,\n",
       " 'exit': 218,\n",
       " 'part': 219,\n",
       " 'still': 220,\n",
       " 'does': 221,\n",
       " 'hee': 222,\n",
       " 'osr': 223,\n",
       " 'long': 224,\n",
       " 'before': 225,\n",
       " 'beleeue': 226,\n",
       " 'any': 227,\n",
       " 'old': 228,\n",
       " 'thoughts': 229,\n",
       " 'first': 230,\n",
       " 'eare': 231,\n",
       " 'keepe': 232,\n",
       " 'goe': 233,\n",
       " 'end': 234,\n",
       " 'guildensterne': 235,\n",
       " 'welcome': 236,\n",
       " 'while': 237,\n",
       " 'art': 238,\n",
       " 'noble': 239,\n",
       " 'body': 240,\n",
       " 'bee': 241,\n",
       " 'daughter': 242,\n",
       " 'speech': 243,\n",
       " 'makes': 244,\n",
       " \"there's\": 245,\n",
       " 'sword': 246,\n",
       " 'stand': 247,\n",
       " 'liue': 248,\n",
       " \"that's\": 249,\n",
       " 'farewell': 250,\n",
       " 'kin': 251,\n",
       " 'ere': 252,\n",
       " 'marry': 253,\n",
       " 'betweene': 254,\n",
       " 'many': 255,\n",
       " 'since': 256,\n",
       " 'watch': 257,\n",
       " \"ha's\": 258,\n",
       " 'therefore': 259,\n",
       " 'question': 260,\n",
       " 'thought': 261,\n",
       " 'heard': 262,\n",
       " 'spirit': 263,\n",
       " 'eye': 264,\n",
       " 'better': 265,\n",
       " 'thine': 266,\n",
       " 'tongue': 267,\n",
       " 'drinke': 268,\n",
       " 'youth': 269,\n",
       " 'sent': 270,\n",
       " 'graue': 271,\n",
       " 'rest': 272,\n",
       " 'bed': 273,\n",
       " 'last': 274,\n",
       " 'same': 275,\n",
       " 'marke': 276,\n",
       " 'gone': 277,\n",
       " 'without': 278,\n",
       " 'state': 279,\n",
       " \"is't\": 280,\n",
       " 'goes': 281,\n",
       " 'fortinbras': 282,\n",
       " 'vse': 283,\n",
       " 'grace': 284,\n",
       " 'euer': 285,\n",
       " 'finde': 286,\n",
       " 'gertrude': 287,\n",
       " 'beare': 288,\n",
       " 'little': 289,\n",
       " 'breath': 290,\n",
       " \"wee'l\": 291,\n",
       " 'saw': 292,\n",
       " 'beene': 293,\n",
       " 'none': 294,\n",
       " 'vertue': 295,\n",
       " 'else': 296,\n",
       " 'said': 297,\n",
       " 'after': 298,\n",
       " 'reynol': 299,\n",
       " 'cause': 300,\n",
       " 'forme': 301,\n",
       " 'something': 302,\n",
       " 'ayre': 303,\n",
       " 'farre': 304,\n",
       " 'selues': 305,\n",
       " 'purpose': 306,\n",
       " 'further': 307,\n",
       " 'reason': 308,\n",
       " 'friend': 309,\n",
       " 'madam': 310,\n",
       " 'remember': 311,\n",
       " 'faith': 312,\n",
       " 'gentlemen': 313,\n",
       " 'word': 314,\n",
       " 'foule': 315,\n",
       " 'winde': 316,\n",
       " 'meane': 317,\n",
       " 'bring': 318,\n",
       " 'fit': 319,\n",
       " 'blood': 320,\n",
       " 'helpe': 321,\n",
       " 'honest': 322,\n",
       " 'stay': 323,\n",
       " \"in't\": 324,\n",
       " 'being': 325,\n",
       " 'fire': 326,\n",
       " 'things': 327,\n",
       " \"what's\": 328,\n",
       " 'newes': 329,\n",
       " 'best': 330,\n",
       " 'kinde': 331,\n",
       " 'excellent': 332,\n",
       " 'each': 333,\n",
       " 'sleepe': 334,\n",
       " 'way': 335,\n",
       " 'please': 336,\n",
       " 'free': 337,\n",
       " 'reuenge': 338,\n",
       " 'villaine': 339,\n",
       " 'right': 340,\n",
       " 'ha': 341,\n",
       " 'passion': 342,\n",
       " 'rosincrance': 343,\n",
       " 'dost': 344,\n",
       " 'verie': 345,\n",
       " 'barn': 346,\n",
       " 'marcellus': 347,\n",
       " 'men': 348,\n",
       " 'peace': 349,\n",
       " 'together': 350,\n",
       " 'full': 351,\n",
       " 'voyce': 352,\n",
       " 'oft': 353,\n",
       " 'greefe': 354,\n",
       " \"'twere\": 355,\n",
       " 'late': 356,\n",
       " 'businesse': 357,\n",
       " 'doubt': 358,\n",
       " 'alone': 359,\n",
       " 'minde': 360,\n",
       " 'heauens': 361,\n",
       " 'face': 362,\n",
       " 'hell': 363,\n",
       " 'ye': 364,\n",
       " 'second': 365,\n",
       " 'iudgement': 366,\n",
       " 'giuen': 367,\n",
       " 'command': 368,\n",
       " 'action': 369,\n",
       " \"let's\": 370,\n",
       " 'murther': 371,\n",
       " 'guil': 372,\n",
       " 'lady': 373,\n",
       " 'fortune': 374,\n",
       " 'mee': 375,\n",
       " 'pyrrhus': 376,\n",
       " 'answer': 377,\n",
       " 'get': 378,\n",
       " 'thankes': 379,\n",
       " 'goodnight': 380,\n",
       " 'eares': 381,\n",
       " 'breake': 382,\n",
       " 'hora': 383,\n",
       " 'strange': 384,\n",
       " 'young': 385,\n",
       " 'walke': 386,\n",
       " 'brothers': 387,\n",
       " 'seeme': 388,\n",
       " 'name': 389,\n",
       " 'fellow': 390,\n",
       " 'act': 391,\n",
       " 'hands': 392,\n",
       " 'armes': 393,\n",
       " 'deare': 394,\n",
       " 'neere': 395,\n",
       " 'phrase': 396,\n",
       " 'draw': 397,\n",
       " 'gho': 398,\n",
       " 'alas': 399,\n",
       " 'ought': 400,\n",
       " 'offence': 401,\n",
       " 'sweare': 402,\n",
       " 'worke': 403,\n",
       " 'gentleman': 404,\n",
       " 'fine': 405,\n",
       " 'three': 406,\n",
       " 'barnardo': 407,\n",
       " 'fran': 408,\n",
       " 'ground': 409,\n",
       " 'sight': 410,\n",
       " 'sit': 411,\n",
       " 'maiesty': 412,\n",
       " 'pale': 413,\n",
       " \"on't\": 414,\n",
       " 'fell': 415,\n",
       " 'lost': 416,\n",
       " 'soft': 417,\n",
       " 'power': 418,\n",
       " 'yong': 419,\n",
       " 'duty': 420,\n",
       " 'whole': 421,\n",
       " 'woe': 422,\n",
       " 'ioy': 423,\n",
       " 'wife': 424,\n",
       " 'came': 425,\n",
       " 'queen': 426,\n",
       " 'seeke': 427,\n",
       " 'common': 428,\n",
       " 'seemes': 429,\n",
       " 'blacke': 430,\n",
       " 'kings': 431,\n",
       " 'teares': 432,\n",
       " 'top': 433,\n",
       " 'fashion': 434,\n",
       " 'deed': 435,\n",
       " 'euery': 436,\n",
       " 'light': 437,\n",
       " 'custome': 438,\n",
       " 'borne': 439,\n",
       " 'wilt': 440,\n",
       " 'hither': 441,\n",
       " 'lay': 442,\n",
       " 'another': 443,\n",
       " 'ouer': 444,\n",
       " 'age': 445,\n",
       " 'thousand': 446,\n",
       " 'fall': 447,\n",
       " 'lye': 448,\n",
       " 'conscience': 449,\n",
       " 'husband': 450,\n",
       " 'bar': 451,\n",
       " 'lookes': 452,\n",
       " 'charge': 453,\n",
       " 'knowne': 454,\n",
       " 'law': 455,\n",
       " 'bin': 456,\n",
       " 'sound': 457,\n",
       " 'sister': 458,\n",
       " 'memory': 459,\n",
       " 'brother': 460,\n",
       " 'beseech': 461,\n",
       " 'lesse': 462,\n",
       " 'dust': 463,\n",
       " 'through': 464,\n",
       " 'shewes': 465,\n",
       " 'desire': 466,\n",
       " 'obey': 467,\n",
       " 'woman': 468,\n",
       " 'almost': 469,\n",
       " 'grow': 470,\n",
       " 'here': 471,\n",
       " 'shame': 472,\n",
       " 'giues': 473,\n",
       " \"too't\": 474,\n",
       " 'takes': 475,\n",
       " 'table': 476,\n",
       " 'sure': 477,\n",
       " 'musicke': 478,\n",
       " 'letters': 479,\n",
       " 'hamlets': 480,\n",
       " 'hope': 481,\n",
       " 'receiue': 482,\n",
       " 'maiestie': 483,\n",
       " 'thanke': 484,\n",
       " 'gaue': 485,\n",
       " 'bad': 486,\n",
       " 'wee': 487,\n",
       " 'ore': 488,\n",
       " 'noise': 489,\n",
       " 'times': 490,\n",
       " 'cold': 491,\n",
       " 'bid': 492,\n",
       " 'dane': 493,\n",
       " 'place': 494,\n",
       " 'peece': 495,\n",
       " 'buried': 496,\n",
       " 'cast': 497,\n",
       " 'hot': 498,\n",
       " 'list': 499,\n",
       " 'wrong': 500,\n",
       " 'sea': 501,\n",
       " 'truth': 502,\n",
       " 'sayes': 503,\n",
       " 'season': 504,\n",
       " 'gracious': 505,\n",
       " 'dumbe': 506,\n",
       " 'loues': 507,\n",
       " 'sorrow': 508,\n",
       " 'marriage': 509,\n",
       " 'writ': 510,\n",
       " 'mouth': 511,\n",
       " 'pardon': 512,\n",
       " 'note': 513,\n",
       " 'backe': 514,\n",
       " 'lordship': 515,\n",
       " 'mothers': 516,\n",
       " 'beard': 517,\n",
       " 'fare': 518,\n",
       " 'seruice': 519,\n",
       " 'withall': 520,\n",
       " 'maid': 521,\n",
       " 'enough': 522,\n",
       " 'effect': 523,\n",
       " 'double': 524,\n",
       " 'neither': 525,\n",
       " 'false': 526,\n",
       " 'vnderstand': 527,\n",
       " 'circumstance': 528,\n",
       " 'foole': 529,\n",
       " 'vowes': 530,\n",
       " 'keepes': 531,\n",
       " 'shape': 532,\n",
       " 'dayes': 533,\n",
       " 'fat': 534,\n",
       " 'crowne': 535,\n",
       " 'wits': 536,\n",
       " 'damned': 537,\n",
       " 'ho': 538,\n",
       " 'needs': 539,\n",
       " 'touch': 540,\n",
       " 'ranke': 541,\n",
       " 'generall': 542,\n",
       " 'moue': 543,\n",
       " 'home': 544,\n",
       " 'ill': 545,\n",
       " 'round': 546,\n",
       " 'fortunes': 547,\n",
       " 'laugh': 548,\n",
       " 'yours': 549,\n",
       " \"he's\": 550,\n",
       " 'honor': 551,\n",
       " 'begin': 552,\n",
       " 'anon': 553,\n",
       " 'proofe': 554,\n",
       " 'gods': 555,\n",
       " 'quicke': 556,\n",
       " 'dangerous': 557,\n",
       " 'christian': 558,\n",
       " 'danish': 559,\n",
       " 'poyson': 560,\n",
       " 'begge': 561,\n",
       " 'wager': 562,\n",
       " \"drown'd\": 563,\n",
       " 'water': 564,\n",
       " 'scull': 565,\n",
       " 'houre': 566,\n",
       " 'twelue': 567,\n",
       " 'quiet': 568,\n",
       " 'course': 569,\n",
       " 'sometimes': 570,\n",
       " 'march': 571,\n",
       " 'look': 572,\n",
       " 'norwey': 573,\n",
       " 'particular': 574,\n",
       " 'land': 575,\n",
       " \"do's\": 576,\n",
       " 'vnto': 577,\n",
       " 'speak': 578,\n",
       " 'spirits': 579,\n",
       " 'cocke': 580,\n",
       " 'guilty': 581,\n",
       " \"'gainst\": 582,\n",
       " 'wholsome': 583,\n",
       " 'lords': 584,\n",
       " 'kingdome': 585,\n",
       " 'freely': 586,\n",
       " 'dreame': 587,\n",
       " 'told': 588,\n",
       " 'loose': 589,\n",
       " 'dread': 590,\n",
       " 'returne': 591,\n",
       " 'france': 592,\n",
       " 'confesse': 593,\n",
       " 'dye': 594,\n",
       " 'visage': 595,\n",
       " 'truly': 596,\n",
       " 'bound': 597,\n",
       " 'prythee': 598,\n",
       " 'health': 599,\n",
       " 'flesh': 600,\n",
       " 'fie': 601,\n",
       " 'beast': 602,\n",
       " 'discourse': 603,\n",
       " 'longer': 604,\n",
       " 'wicked': 605,\n",
       " 'disposition': 606,\n",
       " 'report': 607,\n",
       " 'teach': 608,\n",
       " 'forth': 609,\n",
       " 'thinkes': 610,\n",
       " 'tis': 611,\n",
       " 'yes': 612,\n",
       " 'countenance': 613,\n",
       " 'perchance': 614,\n",
       " 'warrant': 615,\n",
       " 'silence': 616,\n",
       " 'perhaps': 617,\n",
       " 'wisedome': 618,\n",
       " 'blessing': 619,\n",
       " 'dull': 620,\n",
       " 'mans': 621,\n",
       " 'audience': 622,\n",
       " \"you'l\": 623,\n",
       " 'making': 624,\n",
       " \"damn'd\": 625,\n",
       " 'soules': 626,\n",
       " 'cries': 627,\n",
       " 'desperate': 628,\n",
       " 'shalt': 629,\n",
       " 'prison': 630,\n",
       " 'went': 631,\n",
       " 'naturall': 632,\n",
       " 'holds': 633,\n",
       " 'sodaine': 634,\n",
       " 'adue': 635,\n",
       " 'braine': 636,\n",
       " 'knaue': 637,\n",
       " 'point': 638,\n",
       " 'vnder': 639,\n",
       " 'mercy': 640,\n",
       " 'lacke': 641,\n",
       " \"heere's\": 642,\n",
       " \"in's\": 643,\n",
       " 'tooke': 644,\n",
       " 'arme': 645,\n",
       " 'brought': 646,\n",
       " 'dutie': 647,\n",
       " 'found': 648,\n",
       " 'whereon': 649,\n",
       " 'commission': 650,\n",
       " 'passe': 651,\n",
       " 'vilde': 652,\n",
       " 'short': 653,\n",
       " 'try': 654,\n",
       " 'behinde': 655,\n",
       " 'presently': 656,\n",
       " 'slaue': 657,\n",
       " 'saue': 658,\n",
       " 'ambition': 659,\n",
       " 'sing': 660,\n",
       " 'themselues': 661,\n",
       " 'braines': 662,\n",
       " \"'twas\": 663,\n",
       " 'onely': 664,\n",
       " 'french': 665,\n",
       " 'treason': 666,\n",
       " 'morrow': 667,\n",
       " 'conceit': 668,\n",
       " 'drowne': 669,\n",
       " 'fellowes': 670,\n",
       " 'hoa': 671,\n",
       " 'patience': 672,\n",
       " 'halfe': 673,\n",
       " 'yeare': 674,\n",
       " 'forgot': 675,\n",
       " 'hence': 676,\n",
       " \"doo't\": 677,\n",
       " \"e'ene\": 678,\n",
       " 'slaine': 679,\n",
       " 'sense': 680,\n",
       " 'buriall': 681,\n",
       " 'alexander': 682,\n",
       " 'osricke': 683,\n",
       " 'carriages': 684,\n",
       " 'foyles': 685,\n",
       " 'hit': 686,\n",
       " 'tragedie': 687,\n",
       " 'sicke': 688,\n",
       " 'meet': 689,\n",
       " 'twice': 690,\n",
       " \"'twill\": 691,\n",
       " 'nights': 692,\n",
       " 'starre': 693,\n",
       " \"t'\": 694,\n",
       " 'figure': 695,\n",
       " 'wonder': 696,\n",
       " 'warlike': 697,\n",
       " 'cannon': 698,\n",
       " 'toward': 699,\n",
       " 'image': 700,\n",
       " 'norway': 701,\n",
       " \"seal'd\": 702,\n",
       " 'lands': 703,\n",
       " 'stood': 704,\n",
       " \"return'd\": 705,\n",
       " 'strong': 706,\n",
       " 'motiue': 707,\n",
       " 'ease': 708,\n",
       " 'treasure': 709,\n",
       " 'stop': 710,\n",
       " 'violence': 711,\n",
       " 'trumpet': 712,\n",
       " 'whether': 713,\n",
       " 'heerein': 714,\n",
       " 'wherein': 715,\n",
       " 'impart': 716,\n",
       " 'morning': 717,\n",
       " 'attendant': 718,\n",
       " 'brow': 719,\n",
       " 'discretion': 720,\n",
       " 'followes': 721,\n",
       " 'frame': 722,\n",
       " 'cornelius': 723,\n",
       " 'giuing': 724,\n",
       " 'commend': 725,\n",
       " \"would'st\": 726,\n",
       " 'natiue': 727,\n",
       " 'fauour': 728,\n",
       " 'hang': 729,\n",
       " 'colour': 730,\n",
       " 'show': 731,\n",
       " 'fault': 732,\n",
       " 'throw': 733,\n",
       " 'lose': 734,\n",
       " 'gentle': 735,\n",
       " 'sits': 736,\n",
       " 'growes': 737,\n",
       " 'visit': 738,\n",
       " 'growne': 739,\n",
       " 'vnkle': 740,\n",
       " 'left': 741,\n",
       " 'speed': 742,\n",
       " 'incestuous': 743,\n",
       " 'glad': 744,\n",
       " 'forget': 745,\n",
       " 'thrift': 746,\n",
       " \"arm'd\": 747,\n",
       " 'thrice': 748,\n",
       " 'length': 749,\n",
       " 'kept': 750,\n",
       " 'knew': 751,\n",
       " 'answere': 752,\n",
       " 'honour': 753,\n",
       " 'wide': 754,\n",
       " 'force': 755,\n",
       " 'affection': 756,\n",
       " 'shot': 757,\n",
       " 'danger': 758,\n",
       " 'moone': 759,\n",
       " 'safety': 760,\n",
       " 'lies': 761,\n",
       " 'buy': 762,\n",
       " 'aboue': 763,\n",
       " 'humbly': 764,\n",
       " 'tend': 765,\n",
       " 'ist': 766,\n",
       " 'bloud': 767,\n",
       " 'heate': 768,\n",
       " 'bones': 769,\n",
       " 'base': 770,\n",
       " 'horrible': 771,\n",
       " 'imagination': 772,\n",
       " 'direct': 773,\n",
       " 'lend': 774,\n",
       " 'hearing': 775,\n",
       " 'certaine': 776,\n",
       " 'fast': 777,\n",
       " 'house': 778,\n",
       " 'tale': 779,\n",
       " 'start': 780,\n",
       " 'vnnaturall': 781,\n",
       " \"it's\": 782,\n",
       " 'sleeping': 783,\n",
       " 'wit': 784,\n",
       " 'gifts': 785,\n",
       " 'seeming': 786,\n",
       " 'wil': 787,\n",
       " 'court': 788,\n",
       " 'cursed': 789,\n",
       " 'instant': 790,\n",
       " 'bosome': 791,\n",
       " 'distracted': 792,\n",
       " 'yea': 793,\n",
       " 'past': 794,\n",
       " 'booke': 795,\n",
       " 'think': 796,\n",
       " 'secret': 797,\n",
       " 'already': 798,\n",
       " 'stage': 799,\n",
       " 'seeing': 800,\n",
       " 'fingers': 801,\n",
       " 'reynoldo': 802,\n",
       " 'drift': 803,\n",
       " 'liberty': 804,\n",
       " 'closes': 805,\n",
       " 'consequence': 806,\n",
       " 'chamber': 807,\n",
       " \"turn'd\": 808,\n",
       " 'extasie': 809,\n",
       " 'violent': 810,\n",
       " 'meant': 811,\n",
       " 'hide': 812,\n",
       " 'whom': 813,\n",
       " 'rather': 814,\n",
       " 'white': 815,\n",
       " 'awhile': 816,\n",
       " 'idle': 817,\n",
       " 'thence': 818,\n",
       " 'bene': 819,\n",
       " 'foure': 820,\n",
       " 'honestie': 821,\n",
       " 'either': 822,\n",
       " 'count': 823,\n",
       " 'dreames': 824,\n",
       " 'bodies': 825,\n",
       " 'comming': 826,\n",
       " 'shal': 827,\n",
       " 'player': 828,\n",
       " 'flourish': 829,\n",
       " 'asse': 830,\n",
       " 'heauy': 831,\n",
       " 'masters': 832,\n",
       " '1': 833,\n",
       " 'others': 834,\n",
       " 'modestie': 835,\n",
       " 'cunning': 836,\n",
       " \"lou'd\": 837,\n",
       " 'horse': 838,\n",
       " 'priam': 839,\n",
       " 'new': 840,\n",
       " 'sleepes': 841,\n",
       " 'hecuba': 842,\n",
       " 'mortall': 843,\n",
       " 'weepe': 844,\n",
       " 'pate': 845,\n",
       " 'diuell': 846,\n",
       " 'blame': 847,\n",
       " 'flye': 848,\n",
       " 'turne': 849,\n",
       " 'beautie': 850,\n",
       " 'acte': 851,\n",
       " 'nunnery': 852,\n",
       " 'snow': 853,\n",
       " 'rose': 854,\n",
       " 'quite': 855,\n",
       " 'ladies': 856,\n",
       " 'wretched': 857,\n",
       " 'send': 858,\n",
       " 'weare': 859,\n",
       " \"kill'd\": 860,\n",
       " 'kill': 861,\n",
       " 'maker': 862,\n",
       " 'prologue': 863,\n",
       " 'shortly': 864,\n",
       " 'begun': 865,\n",
       " 'lights': 866,\n",
       " 'heeles': 867,\n",
       " 'doore': 868,\n",
       " 'meete': 869,\n",
       " 'stronger': 870,\n",
       " 'whereto': 871,\n",
       " 'next': 872,\n",
       " 'messenger': 873,\n",
       " 'choose': 874,\n",
       " 'bore': 875,\n",
       " 'spade': 876,\n",
       " 'gallowes': 877,\n",
       " 'sings': 878,\n",
       " 'rites': 879,\n",
       " \"woo't\": 880,\n",
       " 'cup': 881,\n",
       " 'vnfold': 882,\n",
       " 'bitter': 883,\n",
       " 'guard': 884,\n",
       " 'mouse': 885,\n",
       " \"appear'd\": 886,\n",
       " 'saies': 887,\n",
       " 'touching': 888,\n",
       " 'along': 889,\n",
       " 'appeare': 890,\n",
       " 'beating': 891,\n",
       " 'offended': 892,\n",
       " 'ambitious': 893,\n",
       " 'iust': 894,\n",
       " 'knowes': 895,\n",
       " 'subiect': 896,\n",
       " 'sore': 897,\n",
       " 'least': 898,\n",
       " 'valiant': 899,\n",
       " 'recouer': 900,\n",
       " 'termes': 901,\n",
       " 'maine': 902,\n",
       " 'loe': 903,\n",
       " 'offer': 904,\n",
       " 'present': 905,\n",
       " 'dew': 906,\n",
       " 'high': 907,\n",
       " 'hill': 908,\n",
       " 'aduice': 909,\n",
       " 'consent': 910,\n",
       " 'scena': 911,\n",
       " 'greene': 912,\n",
       " 'hearts': 913,\n",
       " 'wisest': 914,\n",
       " 'remembrance': 915,\n",
       " 'funerall': 916,\n",
       " 'delight': 917,\n",
       " 'affaire': 918,\n",
       " 'thinking': 919,\n",
       " 'voltemand': 920,\n",
       " 'vncle': 921,\n",
       " 'heartily': 922,\n",
       " 'bend': 923,\n",
       " 'bow': 924,\n",
       " 'cosin': 925,\n",
       " 'clouds': 926,\n",
       " 'sun': 927,\n",
       " 'liues': 928,\n",
       " 'passing': 929,\n",
       " 'formes': 930,\n",
       " 'vnderstanding': 931,\n",
       " 'sence': 932,\n",
       " 'coarse': 933,\n",
       " 'dyed': 934,\n",
       " 'beares': 935,\n",
       " 'wittenberg': 936,\n",
       " 'courtier': 937,\n",
       " 'louing': 938,\n",
       " 'smiling': 939,\n",
       " 'manet': 940,\n",
       " 'fixt': 941,\n",
       " 'flat': 942,\n",
       " 'vses': 943,\n",
       " 'married': 944,\n",
       " 'hercules': 945,\n",
       " 'salt': 946,\n",
       " 'change': 947,\n",
       " 'deepe': 948,\n",
       " 'mock': 949,\n",
       " 'hard': 950,\n",
       " 'tables': 951,\n",
       " 'goodly': 952,\n",
       " 'admiration': 953,\n",
       " 'deliuer': 954,\n",
       " 'cap': 955,\n",
       " 'appeares': 956,\n",
       " 'whilst': 957,\n",
       " 'dreadfull': 958,\n",
       " 'third': 959,\n",
       " 'sirs': 960,\n",
       " 'foote': 961,\n",
       " 'staid': 962,\n",
       " 'assume': 963,\n",
       " 'person': 964,\n",
       " 'mens': 965,\n",
       " 'choyce': 966,\n",
       " 'beauty': 967,\n",
       " 'spring': 968,\n",
       " 'aboord': 969,\n",
       " 'saile': 970,\n",
       " 'few': 971,\n",
       " 'steele': 972,\n",
       " 'entertainment': 973,\n",
       " 'censure': 974,\n",
       " 'rich': 975,\n",
       " 'generous': 976,\n",
       " 'edge': 977,\n",
       " 'tenders': 978,\n",
       " 'pay': 979,\n",
       " 'tender': 980,\n",
       " 'honourable': 981,\n",
       " 'presence': 982,\n",
       " 'meere': 983,\n",
       " 'wayes': 984,\n",
       " 'hower': 985,\n",
       " 'strooke': 986,\n",
       " 'wont': 987,\n",
       " \"honour'd\": 988,\n",
       " 'angels': 989,\n",
       " 'royall': 990,\n",
       " 'burst': 991,\n",
       " 'ignorance': 992,\n",
       " 'shake': 993,\n",
       " 'wherefore': 994,\n",
       " 'tempt': 995,\n",
       " 'lets': 996,\n",
       " 'lead': 997,\n",
       " 'crimes': 998,\n",
       " 'starres': 999,\n",
       " 'stirre': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/hamlet.txt','r') as file_obj:\n",
    "    text = file_obj.read().lower()\n",
    "\n",
    "# creating indexes for words \n",
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(total_words)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[the tragedie of hamlet by william shakespeare 1599]\n",
      "[[1, 687, 4, 45, 41, 1886, 1887, 1888]]\n",
      "[1, 687, 4, 45, 41, 1886, 1887, 1888]\n",
      "[[1, 687], [1, 687, 4], [1, 687, 4, 45], [1, 687, 4, 45, 41], [1, 687, 4, 45, 41, 1886], [1, 687, 4, 45, 41, 1886, 1887], [1, 687, 4, 45, 41, 1886, 1887, 1888]]\n"
     ]
    }
   ],
   "source": [
    "inputSequences=[]\n",
    "\n",
    "# for each line we are taking 0, 0 1, 0 1 2, 0 1 2 3 sequences to train our model\n",
    "\n",
    "for line in text.split('\\n'):\n",
    "    print(line)\n",
    "    print(tokenizer.texts_to_sequences([line]))\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    print(token_list)\n",
    "    for i in range(1,len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        inputSequences.append(n_gram_sequence)\n",
    "    print(inputSequences)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSequences=[]\n",
    "\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1,len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        inputSequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "## Pad Sequences\n",
    "maxSeqLen = max([len(x) for x in inputSequences])\n",
    "print(maxSeqLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    1,  687],\n",
       "       [   0,    0,    0, ...,    1,  687,    4],\n",
       "       [   0,    0,    0, ...,  687,    4,   45],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    4,   45, 1047],\n",
       "       [   0,    0,    0, ...,   45, 1047,    4],\n",
       "       [   0,    0,    0, ..., 1047,    4,  193]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(pad_sequences(inputSequences,maxlen=maxSeqLen)))\n",
    "inputSequences = np.array(pad_sequences(inputSequences,maxlen=maxSeqLen,padding='pre'))\n",
    "inputSequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X,y = inputSequences[:,:-1],inputSequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 687    4   45 ... 1047    4  193]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# y to be converted into categorical variable\n",
    "# why ?\n",
    "\n",
    "# here each index is represented by some word\n",
    "# as out output will be the probablity of the word being among all \n",
    "# the unique words present in the word_list obtained from the text.\n",
    "\n",
    "print(y)\n",
    "y = tf.keras.utils.to_categorical(y,num_classes=total_words)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33,random_state=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the Parameters:\n",
    "\n",
    "1) total_words: \n",
    "- Represents the vocabulary size, i.e., the total number of unique words in your text data. \n",
    "- Each word in the input sequence will be mapped to a corresponding vector of fixed size.\n",
    "\n",
    "2) 100:\n",
    "- This is the embedding dimension, i.e., the size of the vector representation of each word.\n",
    "- Each word in your vocabulary will be represented as a vector of 100 numbers in the embedding space.\n",
    "- Yes, this represents the number of features for a word. It determines how much information you want to encode in the vector representation of a word.\n",
    "\n",
    "3) input_length=maxSeqLen:\n",
    "- Refers to the length of input sequences to the model.\n",
    "- If your input data consists of sequences of words (e.g., sentences or text chunks), maxSeqLen is the fixed length of these sequences.\n",
    "- For instance, if your sentences have 10 words, then maxSeqLen would be 10.\n",
    "\n",
    "\n",
    "### What Does the Embedding Layer Do?  \n",
    "\n",
    "- The embedding layer converts integer-encoded words (word indices) into dense vectors of fixed size (100 in this case).  \n",
    "- It initializes the word embeddings randomly and updates them during training, learning a meaningful representation of words in the process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Layer\n",
    "\n",
    "- The Dropout layer is applied to the outputs of the first LSTM layer.\n",
    "\n",
    "- After the first LSTM layer produces its outputs (a sequence of vectors because return_sequences=True), the Dropout layer randomly sets 20% (0.2) of those outputs to zero during training to prevent overfitting.\n",
    "\n",
    "- LSTM itself has built-in arguments for dropout within its gates and recurrent connections:  \n",
    "    1) dropout: Applies dropout to the input connections of the LSTM.  \n",
    "    2) recurrent_dropout: Applies dropout to the recurrent connections (connections between time steps).  \n",
    "\n",
    "    model.add(LSTM(150, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 13, 100)           481800    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 13, 150)           150600    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 13, 150)           0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 100)               100400    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4818)              486618    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1219418 (4.65 MB)\n",
      "Trainable params: 1219418 (4.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Training our LSTM RNN \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "earlyStoppingCallback = EarlyStopping(monitor='val_accuracy',patience=5,min_delta=0.00001,restore_best_weights=True)\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Embedding(total_words,100,input_length=maxSeqLen-1)) # as the last word is to be predicted\n",
    "model.add(LSTM(150,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words,activation='softmax'))\n",
    "\n",
    "# set the optimizer and the loss function\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "539/539 [==============================] - 11s 21ms/step - loss: 5.1751 - accuracy: 0.1020 - val_loss: 7.6147 - val_accuracy: 0.0595\n",
      "Epoch 2/150\n",
      "539/539 [==============================] - 13s 25ms/step - loss: 5.0494 - accuracy: 0.1060 - val_loss: 7.7741 - val_accuracy: 0.0618\n",
      "Epoch 3/150\n",
      "539/539 [==============================] - 14s 26ms/step - loss: 4.9356 - accuracy: 0.1131 - val_loss: 7.9525 - val_accuracy: 0.0629\n",
      "Epoch 4/150\n",
      "539/539 [==============================] - 14s 26ms/step - loss: 4.8212 - accuracy: 0.1201 - val_loss: 8.0815 - val_accuracy: 0.0626\n",
      "Epoch 5/150\n",
      "539/539 [==============================] - 14s 26ms/step - loss: 4.7104 - accuracy: 0.1244 - val_loss: 8.1842 - val_accuracy: 0.0634\n",
      "Epoch 6/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 4.5979 - accuracy: 0.1320 - val_loss: 8.2723 - val_accuracy: 0.0638\n",
      "Epoch 7/150\n",
      "539/539 [==============================] - 16s 29ms/step - loss: 4.4913 - accuracy: 0.1367 - val_loss: 8.4567 - val_accuracy: 0.0608\n",
      "Epoch 8/150\n",
      "539/539 [==============================] - 16s 29ms/step - loss: 4.3817 - accuracy: 0.1433 - val_loss: 8.5877 - val_accuracy: 0.0608\n",
      "Epoch 9/150\n",
      "539/539 [==============================] - 16s 29ms/step - loss: 4.2904 - accuracy: 0.1499 - val_loss: 8.7049 - val_accuracy: 0.0589\n",
      "Epoch 10/150\n",
      "539/539 [==============================] - 18s 34ms/step - loss: 4.1952 - accuracy: 0.1612 - val_loss: 8.8503 - val_accuracy: 0.0575\n",
      "Epoch 11/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 4.1057 - accuracy: 0.1718 - val_loss: 8.9903 - val_accuracy: 0.0591\n",
      "Epoch 12/150\n",
      "539/539 [==============================] - 15s 28ms/step - loss: 4.0249 - accuracy: 0.1816 - val_loss: 9.0637 - val_accuracy: 0.0572\n",
      "Epoch 13/150\n",
      "539/539 [==============================] - 18s 33ms/step - loss: 3.9288 - accuracy: 0.1928 - val_loss: 9.2293 - val_accuracy: 0.0591\n",
      "Epoch 14/150\n",
      "539/539 [==============================] - 16s 30ms/step - loss: 3.8532 - accuracy: 0.2099 - val_loss: 9.3487 - val_accuracy: 0.0592\n",
      "Epoch 15/150\n",
      "539/539 [==============================] - 15s 28ms/step - loss: 3.7806 - accuracy: 0.2224 - val_loss: 9.4605 - val_accuracy: 0.0573\n",
      "Epoch 16/150\n",
      "539/539 [==============================] - 20s 36ms/step - loss: 3.7086 - accuracy: 0.2349 - val_loss: 9.5571 - val_accuracy: 0.0568\n",
      "Epoch 17/150\n",
      "539/539 [==============================] - 22s 42ms/step - loss: 3.6316 - accuracy: 0.2476 - val_loss: 9.6786 - val_accuracy: 0.0584\n",
      "Epoch 18/150\n",
      "539/539 [==============================] - 16s 29ms/step - loss: 3.5691 - accuracy: 0.2591 - val_loss: 9.7974 - val_accuracy: 0.0569\n",
      "Epoch 19/150\n",
      "539/539 [==============================] - 19s 35ms/step - loss: 3.5074 - accuracy: 0.2702 - val_loss: 9.9083 - val_accuracy: 0.0564\n",
      "Epoch 20/150\n",
      "539/539 [==============================] - 18s 33ms/step - loss: 3.4428 - accuracy: 0.2798 - val_loss: 9.9836 - val_accuracy: 0.0556\n",
      "Epoch 21/150\n",
      "539/539 [==============================] - 19s 35ms/step - loss: 3.3830 - accuracy: 0.2896 - val_loss: 10.0996 - val_accuracy: 0.0553\n",
      "Epoch 22/150\n",
      "539/539 [==============================] - 18s 33ms/step - loss: 3.3226 - accuracy: 0.3006 - val_loss: 10.1786 - val_accuracy: 0.0570\n",
      "Epoch 23/150\n",
      "539/539 [==============================] - 18s 33ms/step - loss: 3.2696 - accuracy: 0.3078 - val_loss: 10.2853 - val_accuracy: 0.0549\n",
      "Epoch 24/150\n",
      "539/539 [==============================] - 18s 33ms/step - loss: 3.2197 - accuracy: 0.3185 - val_loss: 10.3427 - val_accuracy: 0.0535\n",
      "Epoch 25/150\n",
      "539/539 [==============================] - 17s 32ms/step - loss: 3.1703 - accuracy: 0.3256 - val_loss: 10.4746 - val_accuracy: 0.0541\n",
      "Epoch 26/150\n",
      "539/539 [==============================] - 17s 31ms/step - loss: 3.1229 - accuracy: 0.3337 - val_loss: 10.5464 - val_accuracy: 0.0553\n",
      "Epoch 27/150\n",
      "539/539 [==============================] - 15s 28ms/step - loss: 3.0756 - accuracy: 0.3431 - val_loss: 10.6179 - val_accuracy: 0.0562\n",
      "Epoch 28/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 3.0240 - accuracy: 0.3498 - val_loss: 10.7042 - val_accuracy: 0.0544\n",
      "Epoch 29/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 2.9868 - accuracy: 0.3619 - val_loss: 10.7908 - val_accuracy: 0.0536\n",
      "Epoch 30/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 2.9429 - accuracy: 0.3677 - val_loss: 10.8558 - val_accuracy: 0.0564\n",
      "Epoch 31/150\n",
      "539/539 [==============================] - 15s 28ms/step - loss: 2.8978 - accuracy: 0.3773 - val_loss: 10.9602 - val_accuracy: 0.0541\n",
      "Epoch 32/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 2.8652 - accuracy: 0.3820 - val_loss: 10.9935 - val_accuracy: 0.0536\n",
      "Epoch 33/150\n",
      "539/539 [==============================] - 15s 29ms/step - loss: 2.8202 - accuracy: 0.3926 - val_loss: 11.0931 - val_accuracy: 0.0519\n",
      "Epoch 34/150\n",
      "539/539 [==============================] - 15s 28ms/step - loss: 2.7834 - accuracy: 0.3984 - val_loss: 11.1451 - val_accuracy: 0.0496\n",
      "Epoch 35/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 2.7495 - accuracy: 0.4039 - val_loss: 11.2342 - val_accuracy: 0.0522\n",
      "Epoch 36/150\n",
      "539/539 [==============================] - 15s 28ms/step - loss: 2.7047 - accuracy: 0.4146 - val_loss: 11.3158 - val_accuracy: 0.0518\n",
      "Epoch 37/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 2.6760 - accuracy: 0.4194 - val_loss: 11.3582 - val_accuracy: 0.0520\n",
      "Epoch 38/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 2.6388 - accuracy: 0.4295 - val_loss: 11.4445 - val_accuracy: 0.0523\n",
      "Epoch 39/150\n",
      "539/539 [==============================] - 18s 33ms/step - loss: 2.6095 - accuracy: 0.4304 - val_loss: 11.5107 - val_accuracy: 0.0519\n",
      "Epoch 40/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 2.5761 - accuracy: 0.4404 - val_loss: 11.5681 - val_accuracy: 0.0526\n",
      "Epoch 41/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 2.5400 - accuracy: 0.4461 - val_loss: 11.6299 - val_accuracy: 0.0505\n",
      "Epoch 42/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 2.5084 - accuracy: 0.4506 - val_loss: 11.7046 - val_accuracy: 0.0517\n",
      "Epoch 43/150\n",
      "539/539 [==============================] - 15s 28ms/step - loss: 2.4669 - accuracy: 0.4584 - val_loss: 11.7542 - val_accuracy: 0.0517\n",
      "Epoch 44/150\n",
      "539/539 [==============================] - 14s 26ms/step - loss: 2.4450 - accuracy: 0.4643 - val_loss: 11.8256 - val_accuracy: 0.0524\n",
      "Epoch 45/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 2.4131 - accuracy: 0.4674 - val_loss: 11.8774 - val_accuracy: 0.0503\n",
      "Epoch 46/150\n",
      "539/539 [==============================] - 14s 26ms/step - loss: 2.3818 - accuracy: 0.4762 - val_loss: 11.9444 - val_accuracy: 0.0520\n",
      "Epoch 47/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 2.3504 - accuracy: 0.4824 - val_loss: 12.0099 - val_accuracy: 0.0509\n",
      "Epoch 48/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 2.3216 - accuracy: 0.4868 - val_loss: 12.0606 - val_accuracy: 0.0462\n",
      "Epoch 49/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 2.2940 - accuracy: 0.4969 - val_loss: 12.1058 - val_accuracy: 0.0523\n",
      "Epoch 50/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 2.2695 - accuracy: 0.5002 - val_loss: 12.2114 - val_accuracy: 0.0491\n",
      "Epoch 51/150\n",
      "539/539 [==============================] - 15s 28ms/step - loss: 2.2361 - accuracy: 0.5064 - val_loss: 12.2399 - val_accuracy: 0.0502\n",
      "Epoch 52/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 2.2194 - accuracy: 0.5056 - val_loss: 12.2897 - val_accuracy: 0.0509\n",
      "Epoch 53/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 2.1883 - accuracy: 0.5161 - val_loss: 12.3504 - val_accuracy: 0.0506\n",
      "Epoch 54/150\n",
      "539/539 [==============================] - 16s 29ms/step - loss: 2.1681 - accuracy: 0.5178 - val_loss: 12.4071 - val_accuracy: 0.0476\n",
      "Epoch 55/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 2.1400 - accuracy: 0.5226 - val_loss: 12.4735 - val_accuracy: 0.0506\n",
      "Epoch 56/150\n",
      "539/539 [==============================] - 15s 28ms/step - loss: 2.1122 - accuracy: 0.5296 - val_loss: 12.5098 - val_accuracy: 0.0475\n",
      "Epoch 57/150\n",
      "539/539 [==============================] - 14s 26ms/step - loss: 2.0880 - accuracy: 0.5362 - val_loss: 12.6175 - val_accuracy: 0.0515\n",
      "Epoch 58/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 2.0642 - accuracy: 0.5405 - val_loss: 12.6282 - val_accuracy: 0.0490\n",
      "Epoch 59/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 2.0465 - accuracy: 0.5430 - val_loss: 12.6505 - val_accuracy: 0.0510\n",
      "Epoch 60/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 2.0192 - accuracy: 0.5459 - val_loss: 12.7204 - val_accuracy: 0.0483\n",
      "Epoch 61/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 2.0005 - accuracy: 0.5517 - val_loss: 12.7414 - val_accuracy: 0.0483\n",
      "Epoch 62/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 1.9778 - accuracy: 0.5575 - val_loss: 12.8147 - val_accuracy: 0.0459\n",
      "Epoch 63/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.9435 - accuracy: 0.5651 - val_loss: 12.9151 - val_accuracy: 0.0490\n",
      "Epoch 64/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.9326 - accuracy: 0.5657 - val_loss: 12.9490 - val_accuracy: 0.0520\n",
      "Epoch 65/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.9078 - accuracy: 0.5747 - val_loss: 12.9719 - val_accuracy: 0.0469\n",
      "Epoch 66/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.8902 - accuracy: 0.5762 - val_loss: 13.0575 - val_accuracy: 0.0489\n",
      "Epoch 67/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.8718 - accuracy: 0.5800 - val_loss: 13.0516 - val_accuracy: 0.0479\n",
      "Epoch 68/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.8458 - accuracy: 0.5855 - val_loss: 13.1389 - val_accuracy: 0.0490\n",
      "Epoch 69/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 1.8229 - accuracy: 0.5911 - val_loss: 13.1903 - val_accuracy: 0.0485\n",
      "Epoch 70/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.8093 - accuracy: 0.5952 - val_loss: 13.2457 - val_accuracy: 0.0465\n",
      "Epoch 71/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.7862 - accuracy: 0.5977 - val_loss: 13.3138 - val_accuracy: 0.0492\n",
      "Epoch 72/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 1.7666 - accuracy: 0.6031 - val_loss: 13.3184 - val_accuracy: 0.0467\n",
      "Epoch 73/150\n",
      "539/539 [==============================] - 14s 26ms/step - loss: 1.7553 - accuracy: 0.6063 - val_loss: 13.3455 - val_accuracy: 0.0483\n",
      "Epoch 74/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.7347 - accuracy: 0.6082 - val_loss: 13.3950 - val_accuracy: 0.0495\n",
      "Epoch 75/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.7088 - accuracy: 0.6165 - val_loss: 13.4807 - val_accuracy: 0.0499\n",
      "Epoch 76/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 1.6950 - accuracy: 0.6176 - val_loss: 13.5042 - val_accuracy: 0.0478\n",
      "Epoch 77/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.6838 - accuracy: 0.6183 - val_loss: 13.5415 - val_accuracy: 0.0471\n",
      "Epoch 78/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.6650 - accuracy: 0.6252 - val_loss: 13.6274 - val_accuracy: 0.0458\n",
      "Epoch 79/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.6430 - accuracy: 0.6284 - val_loss: 13.6817 - val_accuracy: 0.0464\n",
      "Epoch 80/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.6262 - accuracy: 0.6298 - val_loss: 13.6950 - val_accuracy: 0.0483\n",
      "Epoch 81/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.6162 - accuracy: 0.6319 - val_loss: 13.7259 - val_accuracy: 0.0470\n",
      "Epoch 82/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.5920 - accuracy: 0.6375 - val_loss: 13.7657 - val_accuracy: 0.0473\n",
      "Epoch 83/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 1.5805 - accuracy: 0.6403 - val_loss: 13.7985 - val_accuracy: 0.0470\n",
      "Epoch 84/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.5710 - accuracy: 0.6469 - val_loss: 13.8521 - val_accuracy: 0.0451\n",
      "Epoch 85/150\n",
      "539/539 [==============================] - 14s 26ms/step - loss: 1.5497 - accuracy: 0.6466 - val_loss: 13.8800 - val_accuracy: 0.0477\n",
      "Epoch 86/150\n",
      "539/539 [==============================] - 14s 26ms/step - loss: 1.5390 - accuracy: 0.6503 - val_loss: 13.9332 - val_accuracy: 0.0472\n",
      "Epoch 87/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.5178 - accuracy: 0.6540 - val_loss: 13.9923 - val_accuracy: 0.0467\n",
      "Epoch 88/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 1.5068 - accuracy: 0.6527 - val_loss: 14.0514 - val_accuracy: 0.0489\n",
      "Epoch 89/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.4892 - accuracy: 0.6621 - val_loss: 14.0602 - val_accuracy: 0.0471\n",
      "Epoch 90/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.4797 - accuracy: 0.6615 - val_loss: 14.1234 - val_accuracy: 0.0463\n",
      "Epoch 91/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.4609 - accuracy: 0.6646 - val_loss: 14.1378 - val_accuracy: 0.0492\n",
      "Epoch 92/150\n",
      "539/539 [==============================] - 14s 26ms/step - loss: 1.4457 - accuracy: 0.6661 - val_loss: 14.1940 - val_accuracy: 0.0465\n",
      "Epoch 93/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.4342 - accuracy: 0.6723 - val_loss: 14.2107 - val_accuracy: 0.0484\n",
      "Epoch 94/150\n",
      "539/539 [==============================] - 15s 28ms/step - loss: 1.4219 - accuracy: 0.6730 - val_loss: 14.2781 - val_accuracy: 0.0463\n",
      "Epoch 95/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 1.4120 - accuracy: 0.6744 - val_loss: 14.3278 - val_accuracy: 0.0479\n",
      "Epoch 96/150\n",
      "539/539 [==============================] - 14s 26ms/step - loss: 1.3885 - accuracy: 0.6805 - val_loss: 14.3447 - val_accuracy: 0.0485\n",
      "Epoch 97/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 1.3780 - accuracy: 0.6838 - val_loss: 14.3517 - val_accuracy: 0.0458\n",
      "Epoch 98/150\n",
      "539/539 [==============================] - 14s 26ms/step - loss: 1.3638 - accuracy: 0.6874 - val_loss: 14.4467 - val_accuracy: 0.0476\n",
      "Epoch 99/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.3592 - accuracy: 0.6846 - val_loss: 14.4905 - val_accuracy: 0.0453\n",
      "Epoch 100/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.3379 - accuracy: 0.6936 - val_loss: 14.5121 - val_accuracy: 0.0469\n",
      "Epoch 101/150\n",
      "539/539 [==============================] - 16s 29ms/step - loss: 1.3262 - accuracy: 0.6930 - val_loss: 14.5410 - val_accuracy: 0.0456\n",
      "Epoch 102/150\n",
      "539/539 [==============================] - 15s 28ms/step - loss: 1.3232 - accuracy: 0.6907 - val_loss: 14.5869 - val_accuracy: 0.0464\n",
      "Epoch 103/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.3136 - accuracy: 0.6984 - val_loss: 14.6691 - val_accuracy: 0.0451\n",
      "Epoch 104/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.3004 - accuracy: 0.6995 - val_loss: 14.6358 - val_accuracy: 0.0464\n",
      "Epoch 105/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 1.2846 - accuracy: 0.7067 - val_loss: 14.7119 - val_accuracy: 0.0449\n",
      "Epoch 106/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.2850 - accuracy: 0.7001 - val_loss: 14.7117 - val_accuracy: 0.0470\n",
      "Epoch 107/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.2705 - accuracy: 0.7060 - val_loss: 14.7919 - val_accuracy: 0.0466\n",
      "Epoch 108/150\n",
      "539/539 [==============================] - 14s 26ms/step - loss: 1.2554 - accuracy: 0.7082 - val_loss: 14.7753 - val_accuracy: 0.0466\n",
      "Epoch 109/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.2446 - accuracy: 0.7121 - val_loss: 14.8173 - val_accuracy: 0.0464\n",
      "Epoch 110/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.2387 - accuracy: 0.7126 - val_loss: 14.8761 - val_accuracy: 0.0440\n",
      "Epoch 111/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 1.2330 - accuracy: 0.7120 - val_loss: 14.8814 - val_accuracy: 0.0451\n",
      "Epoch 112/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.2193 - accuracy: 0.7171 - val_loss: 14.9669 - val_accuracy: 0.0457\n",
      "Epoch 113/150\n",
      "539/539 [==============================] - 14s 26ms/step - loss: 1.2031 - accuracy: 0.7177 - val_loss: 14.9726 - val_accuracy: 0.0456\n",
      "Epoch 114/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.1972 - accuracy: 0.7197 - val_loss: 15.0477 - val_accuracy: 0.0459\n",
      "Epoch 115/150\n",
      "539/539 [==============================] - 14s 26ms/step - loss: 1.1891 - accuracy: 0.7226 - val_loss: 15.0956 - val_accuracy: 0.0460\n",
      "Epoch 116/150\n",
      "539/539 [==============================] - 16s 30ms/step - loss: 1.1786 - accuracy: 0.7251 - val_loss: 15.0835 - val_accuracy: 0.0477\n",
      "Epoch 117/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 1.1676 - accuracy: 0.7244 - val_loss: 15.1404 - val_accuracy: 0.0450\n",
      "Epoch 118/150\n",
      "539/539 [==============================] - 15s 28ms/step - loss: 1.1576 - accuracy: 0.7303 - val_loss: 15.1726 - val_accuracy: 0.0450\n",
      "Epoch 119/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.1611 - accuracy: 0.7307 - val_loss: 15.2368 - val_accuracy: 0.0426\n",
      "Epoch 120/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 1.1471 - accuracy: 0.7322 - val_loss: 15.2289 - val_accuracy: 0.0458\n",
      "Epoch 121/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.1302 - accuracy: 0.7381 - val_loss: 15.2753 - val_accuracy: 0.0452\n",
      "Epoch 122/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.1310 - accuracy: 0.7372 - val_loss: 15.3181 - val_accuracy: 0.0416\n",
      "Epoch 123/150\n",
      "539/539 [==============================] - 15s 27ms/step - loss: 1.1234 - accuracy: 0.7340 - val_loss: 15.3333 - val_accuracy: 0.0452\n",
      "Epoch 124/150\n",
      "539/539 [==============================] - 15s 29ms/step - loss: 1.1085 - accuracy: 0.7392 - val_loss: 15.3542 - val_accuracy: 0.0450\n",
      "Epoch 125/150\n",
      "539/539 [==============================] - 14s 26ms/step - loss: 1.1002 - accuracy: 0.7410 - val_loss: 15.4089 - val_accuracy: 0.0446\n",
      "Epoch 126/150\n",
      "539/539 [==============================] - 14s 26ms/step - loss: 1.0943 - accuracy: 0.7436 - val_loss: 15.4657 - val_accuracy: 0.0447\n",
      "Epoch 127/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.0862 - accuracy: 0.7430 - val_loss: 15.4681 - val_accuracy: 0.0425\n",
      "Epoch 128/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.0796 - accuracy: 0.7450 - val_loss: 15.5115 - val_accuracy: 0.0458\n",
      "Epoch 129/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.0716 - accuracy: 0.7492 - val_loss: 15.5634 - val_accuracy: 0.0464\n",
      "Epoch 130/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.0625 - accuracy: 0.7524 - val_loss: 15.5382 - val_accuracy: 0.0458\n",
      "Epoch 131/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.0518 - accuracy: 0.7520 - val_loss: 15.6428 - val_accuracy: 0.0451\n",
      "Epoch 132/150\n",
      "539/539 [==============================] - 14s 26ms/step - loss: 1.0468 - accuracy: 0.7547 - val_loss: 15.6545 - val_accuracy: 0.0443\n",
      "Epoch 133/150\n",
      "539/539 [==============================] - 14s 26ms/step - loss: 1.0344 - accuracy: 0.7548 - val_loss: 15.6529 - val_accuracy: 0.0469\n",
      "Epoch 134/150\n",
      "539/539 [==============================] - 14s 26ms/step - loss: 1.0358 - accuracy: 0.7506 - val_loss: 15.7438 - val_accuracy: 0.0476\n",
      "Epoch 135/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.0270 - accuracy: 0.7604 - val_loss: 15.6784 - val_accuracy: 0.0457\n",
      "Epoch 136/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.0213 - accuracy: 0.7592 - val_loss: 15.7863 - val_accuracy: 0.0457\n",
      "Epoch 137/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.0128 - accuracy: 0.7603 - val_loss: 15.7733 - val_accuracy: 0.0460\n",
      "Epoch 138/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 1.0067 - accuracy: 0.7626 - val_loss: 15.8456 - val_accuracy: 0.0427\n",
      "Epoch 139/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 0.9995 - accuracy: 0.7637 - val_loss: 15.8698 - val_accuracy: 0.0451\n",
      "Epoch 140/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 0.9903 - accuracy: 0.7691 - val_loss: 15.8659 - val_accuracy: 0.0450\n",
      "Epoch 141/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 0.9832 - accuracy: 0.7658 - val_loss: 15.9353 - val_accuracy: 0.0447\n",
      "Epoch 142/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 0.9851 - accuracy: 0.7668 - val_loss: 15.9344 - val_accuracy: 0.0467\n",
      "Epoch 143/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 0.9757 - accuracy: 0.7685 - val_loss: 15.9677 - val_accuracy: 0.0456\n",
      "Epoch 144/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 0.9707 - accuracy: 0.7684 - val_loss: 16.0059 - val_accuracy: 0.0464\n",
      "Epoch 145/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 0.9647 - accuracy: 0.7705 - val_loss: 16.0688 - val_accuracy: 0.0446\n",
      "Epoch 146/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 0.9683 - accuracy: 0.7657 - val_loss: 16.0690 - val_accuracy: 0.0444\n",
      "Epoch 147/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 0.9526 - accuracy: 0.7726 - val_loss: 16.0738 - val_accuracy: 0.0440\n",
      "Epoch 148/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 0.9462 - accuracy: 0.7752 - val_loss: 16.0989 - val_accuracy: 0.0440\n",
      "Epoch 149/150\n",
      "539/539 [==============================] - 14s 27ms/step - loss: 0.9407 - accuracy: 0.7778 - val_loss: 16.1558 - val_accuracy: 0.0446\n",
      "Epoch 150/150\n",
      "539/539 [==============================] - 14s 26ms/step - loss: 0.9363 - accuracy: 0.7759 - val_loss: 16.1585 - val_accuracy: 0.0442\n"
     ]
    }
   ],
   "source": [
    "# train the model \n",
    "history = model.fit(X_train,y_train,epochs=150,validation_data=(X_test,y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Model Output:\n",
    "\n",
    "1) Model Output Shape:\n",
    "\n",
    "    - Your model's final layer is a Dense(total_words, activation='softmax').  \n",
    "    - This means the model predicts a probability distribution over total_words (the size of your vocabulary).  \n",
    "    - For each input sequence (N words) in token_list, the model returns a vector of probabilities (length: total_words).  \n",
    "\n",
    "2) Shape of y_pred:\n",
    "\n",
    "    If token_list contains 1 sequence (N words), the shape of the output is (1, total_words):\n",
    "    - N: Number of words in the sequences (batch size).\n",
    "    - total_words: Vocabulary size (number of classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model, tokenizer, text, maxSeqLen):\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0] # converts text to vocabulary indexes\n",
    "    if len(token_list) >= maxSeqLen:\n",
    "        token_list = token_list[-(maxSeqLen-1):] # taking last maxSeqLen-1 words\n",
    "    else:\n",
    "        token_list = pad_sequences([token_list],maxlen=maxSeqLen-1,padding='pre')\n",
    "    \n",
    "    y_pred = model.predict(token_list,verbose=0) # 2d arr of vocabulary size\n",
    "    print(\"the predicted values: \",y_pred)\n",
    "    print(\"Length of values: \",len(y_pred[0]))\n",
    "\n",
    "\n",
    "    predicted_word_index = np.argmax(y_pred,axis=1)\n",
    "\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_word_index:\n",
    "            return word\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are sequences ?\n",
    "\n",
    "A sequence refers to a list (or array) of numbers, where:  \n",
    "1) Each number corresponds to a word or token in your vocabulary.\n",
    "2) A sequence typically represents a part of your input text (e.g., a sentence, phrase, or a fixed-length chunk of text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: I am going to\n",
      "14\n",
      "the predicted values:  [[5.4742877e-10 2.0345326e-03 1.1209423e-04 ... 6.9300030e-25\n",
      "  5.8657768e-10 5.2906424e-10]]\n",
      "Length of values:  4818\n",
      "his\n"
     ]
    }
   ],
   "source": [
    "input_text = \"I am going to\"\n",
    "# input_text = \"I haue seene\"\n",
    "# input_text = \"I thinke I heare\"\n",
    "print(f\"Input text: {input_text}\")\n",
    "\n",
    "max_sequence_len = model.input_shape[1]+1\n",
    "print(max_sequence_len)\n",
    "next_word = predict_next_word(model,tokenizer,input_text.lower(),max_sequence_len)\n",
    "print(next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nitin Flavier\\Desktop\\Data Nexus\\Data Science\\ML_BootCamp\\NLP_Deep_Learning\\Next_Word_Prediction_LSTM\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model \n",
    "model.save('../pickle_files/next_word_lstm.h5')\n",
    "\n",
    "# save the tokenizer\n",
    "with open('../pickle_files/tokenizer.pkl','wb') as file_obj:\n",
    "    pickle.dump(tokenizer,file_obj,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
